---
title: "question8"
output: html_document
---

```{r question8, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(class)
library(caret)
library(e1071)
library(pracma)
library(knitr)
library(kableExtra)

# shared variables
q8_colors = c("#C8102E", "#13294b")
iterations <- 100
positive_value <- 'Other'
```

# Question 8


## Categorize Ales...

```{r}
# categorize beers as ipa, or other ale (still have orig columns as well as infilled)
beers_categorized <- merged %>%
  mutate(is_ipa = grepl("\\bIPA\\b",Style,ignore.case = TRUE)) %>%
  mutate(is_ale = grepl("\\bale\\b",Style,ignore.case = TRUE)) %>%
  mutate(is_american = grepl("\\bamerican\\b",Style,ignore.case = TRUE)) %>%
  mutate(is_belgian = grepl("\\bbelgian\\b",Style,ignore.case = TRUE)) %>%
  mutate(is_other_ale = (is_ale & !is_ipa)) 

# keep only the ales
only_ales <- beers_categorized %>% filter(is_ipa | is_other_ale)

# and make a factor for the type
# here I replace orig columns with infilled columns.
infill_normalized <- only_ales %>% 
  filter(is_ipa | is_other_ale) %>%
  mutate(category = as.factor(ifelse(is_ipa,'IPA','Other'))) %>%
  select(-IBU,-ABV) %>%
  rename(IBU=IBUfill,ABV=ABVfill) %>%
  mutate(abv_z = scale(ABV), ibu_z = scale(IBU)) %>%
  select(ABV,IBU,abv_z,ibu_z,category)

# synthetic data, for looking at predictions 'everywhere'
dummy <- data.frame(ABV=rep(0,100*100),IBU=rep(0,100*100))
ABV <- linspace(min(infill_normalized$ABV),max(infill_normalized$ABV),100)
IBU <- linspace(min(infill_normalized$IBU),max(infill_normalized$IBU),100)
cnt <- 0
for (i in 1:100)
{
  for (j in 1:100)
  {
    cnt <- cnt + 1
    dummy[cnt,] <- c(ABV[i],IBU[j])
  }
}
synth <- dummy %>% mutate(abv_z = scale(ABV), ibu_z = scale(IBU))

# here I leave orig columns alone, but drop observations with missing values.
for_orig_plot <- only_ales %>% 
  filter(is_ipa | is_other_ale) %>%
  mutate(category = as.factor(ifelse(is_ipa,'IPA','Other'))) %>%
  drop_na(ABV,IBU)

# columns to use for predictors
cols_for_knn = c("abv_z","ibu_z")                                                

# counts
total_count <- nrow(beers_raw)
ipa_count <- only_ales %>% filter(is_ipa) %>% nrow()
other_count <- only_ales %>% filter(is_other_ale) %>% nrow()
ale_count <- ipa_count + other_count

ale_percent   <- round(100 * ale_count / total_count)
ipa_percent   <- round(100 * ipa_count / total_count)
other_percent <- round(100 * other_count / total_count)
ipa_percent_of_ales <- round(100 * ipa_count / ale_count)
other_percent_of_ales <- round(100 * other_count / ale_count)
```

Out of a total of `r total_count` beers, `r ipa_count` are IPA and `r other_count` are other ales.

`r ale_percent` % of beers are ales.

`r ipa_percent` % of beers are IPA.

`r ipa_percent_of_ales` % of ales are IPA.


## Make a scatter plot of IBU vs ABV, with only IPA's and other ales

```{r}
for_orig_plot %>% ggplot(aes(x=IBU,y=ABV,color=category)) + 
  geom_point() + ggtitle('ABV vs IBU for IPAs and other ales (missing values dropped)') +
  coord_cartesian(ylim = c(0.02,0.1),xlim=c(0,150)) +
  scale_color_manual(breaks = c("IPA", "Other"),
                        values=q8_colors)
```

## Make the same scatter plot, but using infilled data.

```{r}
infill_normalized %>% ggplot(aes(x=IBU,y=ABV,color=category)) + 
  geom_point() + ggtitle('ABV vs IBU for IPAs and other ales (infilled)') +
  coord_cartesian(ylim = c(0.02,0.1),xlim=c(0,150)) +
  scale_color_manual(breaks = c("IPA", "Other"),
                        values=q8_colors)
```

From these scatter plots, it appears that the character of the relationship between
ABV and IBU, for both IPA's and other ales, is unaffected by the infilling.  This
gives us confidence that the infilling was valid, so further analysis will use only
the infilled  values.

We know that `r ipa_percent_of_ales` % of ales are IPA, so a fixed guess that any 
given Ale is not IPA is the simplest model.  We expect this predictor 
will have an accuracy of `r other_percent_of_ales` %.  It would 
have a sensitivity of 100 % but specificity of 0 %.

## Make a constant classifier and check its performance.

```{r}
size = nrow(infill_normalized)
always_ipa <- data.frame(sample(x=as.factor(c('IPA','Other')),size,replace=TRUE,prob=c(0,1)))
colnames(always_ipa) <- c('guess')
CM_always <- confusionMatrix(always_ipa$guess,infill_normalized$category,positive=positive_value)
always_acc <- CM_always$overall[1] # accuracy
always_sens <- CM_always$byClass[1] # sensitivity
always_spec <- CM_always$byClass[2] # specificity
```

The accuracy turns out to be `r format(100*always_acc,digits=2)` %.

The sensitivity is `r format(100*always_sens,digits=2)` %.

The specificity is `r format(100*always_spec,digits=2)` %.

## Make a random guess classifier and check its performance

A slightly more sophisticated model would be to randomly guess that a beer is an
ale `r other_percent_of_ales` % of the time.  We simulate this to see how such a
'guessing' model would perform.

Try it 100 times and find its average performance.

```{r}
acc_guess <- matrix(nrow=iterations,ncol=3)
size = nrow(infill_normalized)
for (j in 1:iterations) 
{
  guess <- data.frame(sample(x=as.factor(c('IPA','Other')),size,replace=TRUE,prob=c(ipa_count,other_count)))
  colnames(guess) <- c('guess')
  CM_guess <- confusionMatrix(guess$guess,infill_normalized$category,positive=positive_value)
  acc_guess[j,1] <- CM_guess$overall[1] # accuracy
  acc_guess[j,2] <- CM_guess$byClass[1] # sensitivity
  acc_guess[j,3] <- CM_guess$byClass[2] # specificity
}
mean_stats <- colMeans(acc_guess)
mean_accuracy    <- mean_stats[1]
mean_sensitivity <- mean_stats[2]
mean_specificity <- mean_stats[3]

guess_acc <- mean_stats[1]
guess_sens <- mean_stats[2]
guess_spec <- mean_stats[3]

```

The random guess model has accuracy of `r format(100*guess_acc,digits=2)` %.

Its sensitivity is `r format(100*guess_sens,digits=2)` %.

Its specificity is `r format(100*guess_spec,digits=2)` %.

## Look at distributions of ABV and IBU

To see if ABV and IBU values can help us predict if an ale is IPA or not, first
look at distributions of these quantities, separated by IPA or other ale.

```{r}
infill_normalized %>% ggplot(aes(x=ABV)) +
  geom_histogram(aes(color=category,fill=category),
                 position="identity",alpha=0.4,
                 show.legend = TRUE,bins=21) +
  ggtitle('histogram of ABV by category (infilled)') +
  scale_color_manual(values=q8_colors) +
  scale_fill_manual(values=q8_colors)
```

```{r}
infill_normalized %>% ggplot(aes(x=IBU)) +
  geom_histogram(aes(color=category,fill=category),
                 position="identity",alpha=0.4,
                 show.legend = TRUE,bins=21) +
  ggtitle('histogram of IBU by category (infilled)') +
  scale_color_manual(values=q8_colors) +
  scale_fill_manual(values=q8_colors)
```

There appears to be clear separation of both ABV and IBU between
IPA's and other ales.  Since the distributions appear close to
normal, and similiar in shape for the two types of ale, we proceed
with t-tests to determine if there is a statistically significant
difference.

```{r}
ibu_ttest <- t.test(IBU ~ category,infill_normalized,var.equal=FALSE)
ibu_ttest
```

```{r}
abv_ttest <- t.test(ABV ~ category,infill_normalized,var.equal=FALSE)
abv_ttest
```

Check statistically significant difference in ABV and IBU between IPA's and other ales.

```{r}
abv_col <- c(abv_ttest$estimate[2],
             abv_ttest$estimate[1])
ibu_col <- c(ibu_ttest$estimate[2],
             ibu_ttest$estimate[1])
hypothesis <- data.frame(abv_col,ibu_col)
colnames(hypothesis) <- c('ABV(%)','IBU(Intl Bitterness Units)')
rownames(hypothesis) <- c('IPA','Other Ales')
kable(hypothesis,digits=4,caption = 'Difference in characteristics between IPAs and other Ales') %>%
  kable_styling(bootstrap_options = c('striped'))

```


Statistical Significance

```{r}
abv_col <- c(format(abv_ttest$p.value,digits=4),
             paste(format(abv_ttest$conf.int[1],digits=3),format(abv_ttest$conf.int[2],digits=4),sep=" .. "))
ibu_col <- c(format(ibu_ttest$p.value,digits=4),
             paste(format(ibu_ttest$conf.int[1],digits=3),format(ibu_ttest$conf.int[2],digits=1),sep=" .. "))
stat_sig <- data.frame(abv_col,ibu_col)
colnames(stat_sig) <- c('ABV','IBU')
rownames(stat_sig) <- c('p-value','95% conf int')
kable(stat_sig,caption = 'Statistical significance from Welch t-test') %>%
  kable_styling()

```



## Create a Naive Bayes Classifier

Since there are statistically signficant differences with ABV and IBU,
we first try a Naive Bayes classifier.

```{r}
pred_indices <- c("ABV","IBU")
acc_nb = matrix(nrow=iterations,ncol=3)
# for repeatability
set.seed(42)
for (j in 1:iterations)
{
  trainIndices = sample(seq(1:length(infill_normalized$ABV)),
                        round(.7*length(infill_normalized$ABV)))
  trainBeer = infill_normalized[trainIndices,]
  testBeer = infill_normalized[-trainIndices,]
  model_nb <- naiveBayes(trainBeer[,pred_indices],trainBeer$category)
  CM_nb <- confusionMatrix(table(predict(model_nb,testBeer[,pred_indices]),
                                as.factor(testBeer$category)),positive=positive_value)
                          
  acc_nb[j,1] <- CM_nb$overall[1] # accuracy
  acc_nb[j,2] <- CM_nb$byClass[1] # sensitivity
  acc_nb[j,3] <- CM_nb$byClass[2] # specificity
}
mean_stats <- colMeans(acc_nb)
nb_acc <- mean_stats[1]
nb_sens <- mean_stats[2]
nb_spec <- mean_stats[3]
```

Averaging over `r iterations` trials, the Naive Bayes classifier has

accuracy of `r format(100*nb_acc,digits=2)` %

sensitivity of `r format(100*nb_sens,digits=2)` %

specificity of `r format(100*nb_spec,digits=2)` %

To understand the classification regions of the Naieve Bayes classifier,
we make a prediction at every point in the ABV vs IBU space.

```{r}
beer_nb <- naiveBayes(infill_normalized[,pred_indices],infill_normalized$category)
nb_predict <- predict(beer_nb, synth)
synth_w_nbpred <- cbind(synth,nb_predict)
synth_w_nbpred %>% ggplot(aes(x=IBU,y=ABV,color=nb_predict)) + geom_point() +
  ggtitle('Class Map from Naive Bayes') +
  scale_color_manual(breaks = c("IPA", "Other"),values=q8_colors)
```

The Naive Bayes classifier draws a clear boundary betweeen two regions.  Ales
with high ABV and IBU are predicted to be IPA.

However, this does not well capture the relationship that is apparent in the
scatter plot of ABV vs IBU for IPA's and other ales.  To better use this
information, we use a KNN classifier, labeling an ale as IPA or not according
to the majority of its nearest neighbors.  To tune the KNN model, we try
several different neighborhood sizes (k).

## Create a KNN Classifer

Make 100 iterations of the model, splitting the data
into training and test datasets each time and averaging the performance
statistics of the model.  Do this for a number of different k values.

```{r}
# for repeatability
set.seed(42)
splitPerc <- .7
numks <- 50
knn_stats = matrix(nrow=numks,ncol=4)
for (i in 1:numks)
{
  acc  = matrix(nrow=iterations,ncol=1)
  sens = matrix(nrow=iterations,ncol=1)
  spec = matrix(nrow=iterations,ncol=1)
  for (j in 1:iterations)
  {
    trainIndices = sample(1:dim(infill_normalized)[1],round(splitPerc * dim(infill_normalized)[1]))
    train = infill_normalized[trainIndices,]
    test = infill_normalized[-trainIndices,]
    classifications = knn(train[,cols_for_knn],
                              test[,cols_for_knn],
                              as.factor(train$category), prob = TRUE, k = i)
    CM = confusionMatrix(table(as.factor(test$category),classifications),positive=positive_value)
    acc[j]  = CM$overall[1]
    sens[j] = CM$byClass[1]
    spec[j] = CM$byClass[2]
  }
  knn_stats[i,1] = i
  knn_stats[i,2] = colMeans(acc)
  knn_stats[i,3] = colMeans(sens)
  knn_stats[i,4] = colMeans(spec)
}

```

Look at all `r numks` values of k to determine what k value gives best performance.

```{r}
stats_frame <- data.frame(knn_stats)
colnames(stats_frame) <- c("k","accuracy","sensitivity","specificity")
for_plot <- reshape2::melt(stats_frame,id.var='k')
for_plot %>% ggplot(aes(x=k,y=value,col=variable)) + 
  geom_line() +
  xlab('k') +
  ylab('percent') +
  ggtitle('Performance of Classifier by K')
```

```{r}
combined_stats <- stats_frame %>% 
  mutate(sums = accuracy + sensitivity + specificity) 

best_k_acc  <- which.max(combined_stats$accuracy)
best_k_sens <- which.max(combined_stats$sensitivity)
best_k_spec <- which.max(combined_stats$specificity)
best_k_combined <- which.max(combined_stats$sums)

chosen_k = best_k_combined

best_acc <-  round(100*combined_stats$accuracy[chosen_k])
best_sens <- round(100*combined_stats$sensitivity[chosen_k])
best_spec <- round(100*combined_stats$specificity[chosen_k])
```

k of `r best_k_acc`  is best for accuracy.


k of `r best_k_sens` is best for sensitivity.
k of `r best_k_spec` is best for specificity.

we choose `r chosen_k` as best overall, since it gives the
highest sum of accuracy, sensitivity, and specificity.

As with the Naive Bayes classifier, to understand the classification
regions of the KNN classifier, we make a prediction at every point
in the ABV vs IBU space.

```{r}
plot_title <- paste('Class map from knn with k of',as.character(chosen_k))
synth_pred_knn <- knn(infill_normalized[,cols_for_knn],
                  synth[,cols_for_knn],
                  as.factor(infill_normalized$category),k=chosen_k)
synth_w_knnpred <- cbind(synth,synth_pred_knn)
synth_w_knnpred %>% ggplot(aes(x=IBU,y=ABV,color=synth_pred_knn)) + geom_point() + ggtitle(plot_title) +
  scale_color_manual(breaks = c("IPA", "Other"),values=q8_colors)
```

This model does capture more of the detailed relationship between ABV and IBU
for IPA's and other ales.

Finally, go back and make predictions of all points in the original dataset.

```{r}
knn_pred <- knn(infill_normalized[,cols_for_knn],
                infill_normalized[,cols_for_knn],
                as.factor(infill_normalized$category),k=chosen_k)
with_pred <- cbind(infill_normalized,knn_pred)
with_pred %>% ggplot(aes(x=IBU,y=ABV,color=knn_pred)) + geom_point() + ggtitle('Predicted Class') +
  scale_color_manual(breaks = c("IPA", "Other"),
                        values=q8_colors)
CM_knn = confusionMatrix(table(as.factor(infill_normalized$category),knn_pred),positive=positive_value)
knn_acc <- CM_knn$overall[1] # accuracy
knn_sens <- CM_knn$byClass[1] # sensitivity
knn_spec <- CM_knn$byClass[2] # specificity
```

KNN accuracy turns out to be `r format(100*knn_acc,digits=2)` %.

KNN sensitivity is `r format(100*knn_sens,digits=2)` %.

KNN specificity is `r format(100*knn_spec,digits=2)` %.

## Summarizing performance of classification models...

```{r}

always_perf <- 100 * c(always_sens,always_spec,always_acc)
guess_perf <-  100 * c(guess_sens,guess_spec,guess_acc)
nb_perf <-     100 * c(nb_sens,nb_spec,nb_acc)
knn_perf <-    100 * c(knn_sens,knn_spec,knn_acc)

performance <- data.frame(always_perf, guess_perf, nb_perf, knn_perf)
colnames(performance) <- c('Constant','Random','Naive Bayes','KNN')
rownames(performance) <- c('Sensitivity','Specificity','Overall Accuracy')

kable(performance,digits=2,caption = 'Performance (in percent) of models with increasing sophistication') %>%
  kable_styling(bootstrap_options = c('striped'))

```
